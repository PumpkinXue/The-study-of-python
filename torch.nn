torch.nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='Zero')

##
用来实现2d卷积操作
##in_channels —— 输入的channels数
out_channels —— 输出的channels数
kernel_size ——卷积核的尺寸，可以是方形卷积核、也可以不是，下边example可以看到
stride —— 步长，用来控制卷积核移动间隔
padding ——输入边沿扩边操作
padding_mode ——扩边的方式
bias ——是否使用偏置(即out = wx+b中的b)
dilation —— 这个参数简单说，设定了取数之间的间隔
##

torch.nn.MaxPool2d()

##
2d池化运算
kernel_size ：表示做最大池化的窗口大小，可以是单个值，也可以是tuple元组
stride ：步长，可以是单个值，也可以是tuple元组
padding ：填充，可以是单个值，也可以是tuple元组
dilation ：控制窗口中元素步幅
return_indices ：布尔类型，返回最大值位置索引
ceil_mode ：布尔类型，为True，用向上取整的方法，计算输出形状；默认是向下取整。
##

torch.nn.Linear(in_features,out_features,bias=True)

##
in_features:输入的个数
out_features:输出的个数
bias:偏置
即为该式：Yj=Wij*Xi+b
##

torch.nn.functional.dropout(input,p=0.5,training=False,inplace=False)

##
以p的概率使一些神经元的输出为0，减少神经元间的相关性来防止过拟合
使用时应需要设置它的training状态参数与模型整体的一致
即training=self.training
##

torch.max(input,dim)

##
输入
input是一个tensor
dim是max函数索引的维度0/1，0表示对列求最大，1表示对行求最大
输出
函数返回两个tensor，第一个tensor是行/列的最大值，第二个tensor是最大值的索引
一般来说，我们不需要知道最大的值（一般是概率），我们只需要知道最大值的索引。
##

tensor.eq()

##
逐一判断相等
如：
import torch
x=tensor([1,2,3])
y=tensor([1,3,3])
print(x.eq(y))
输出：
tensor([True,False,True])
##

tensor.sum()

##
将所有值相加，得到的仍为tensor
如：
b=torch.Tensor([1,2,3])
b.sum()
输出：
tensor(6.)
##
